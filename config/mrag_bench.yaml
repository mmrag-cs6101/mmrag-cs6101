# MRAG-Bench Reproduction System Configuration
#
# This configuration file defines all parameters for reproducing MRAG-Bench
# baseline results on perspective change scenarios.

model:
  # Model selection: "llava-1.5-7b" (baseline), "llava-onevision-7b" (improved)
  model_type: "llava-onevision-7b"

  # Model paths (mapped from model_type)
  vlm_name: "llava-hf/llava-1.5-7b-hf"
  retriever_name: "openai/clip-vit-base-patch32"

  quantization: "4bit"  # Options: "4bit" (memory efficient), "8bit" (balanced), "none" (full precision)
  max_memory_gb: 14.0
  device: "cuda"
  torch_dtype: "float16"

dataset:
  data_path: "data/mrag_bench"
  batch_size: 4
  image_size: [224, 224]
  cache_embeddings: true
  embedding_cache_path: "data/embeddings"

retrieval:
  embedding_dim: 512
  top_k: 10  # Increased from 5 to 10 for better context
  similarity_threshold: 0.0
  batch_size: 16
  faiss_index_type: "IVF"
  index_cache_path: "data/embeddings/faiss_index.bin"

generation:
  max_length: 10  # Very short - just need a few tokens for answer
  temperature: 0.1  # Very low for deterministic output
  do_sample: false  # Greedy decoding
  top_p: 0.9
  top_k: 50
  num_beams: 1

evaluation:
  scenarios:
    - "angle"
    - "partial"
    - "scope"
    - "occlusion"
  save_results: true
  results_path: "results/evaluation_results.json"
  log_failed_questions: true

performance:
  memory_limit_gb: 16.0
  memory_buffer_gb: 1.0
  retrieval_timeout: 5.0
  generation_timeout: 25.0
  total_pipeline_timeout: 30.0
  enable_memory_monitoring: true